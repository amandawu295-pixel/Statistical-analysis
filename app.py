# app.py
# -*- coding: utf-8 -*-
import io
import os
import re
import traceback

import numpy as np
import pandas as pd
import streamlit as st
import statsmodels.api as sm

from analysis import run_item_analysis, normalize_item_columns


# ---- Optional GPT report (if gpt_report.py exists & has generate_gpt_report) ----
GPT_AVAILABLE = False
generate_gpt_report = None
try:
    from gpt_report import generate_gpt_report  # type: ignore
    GPT_AVAILABLE = callable(generate_gpt_report)
except Exception:
    GPT_AVAILABLE = False
    generate_gpt_report = None


# ---- Page ----
st.set_page_config(page_title="Scale Item Analysis MVP", layout="wide")
st.title("ğŸ“Š Scale Item Analysis MVP")


# ---- Helpers ----
def read_csv_safely(uploaded_file) -> pd.DataFrame:
    """
    Robust CSV loader for Streamlit UploadedFile.
    Tries common encodings and handles BOM.
    """
    if uploaded_file is None:
        raise ValueError("å°šæœªä¸Šå‚³ CSV æª”æ¡ˆã€‚")

    raw = uploaded_file.getvalue()
    if raw is None or len(raw) == 0:
        raise ValueError("ä¸Šå‚³çš„æª”æ¡ˆæ˜¯ç©ºçš„ï¼ˆ0 bytesï¼‰ã€‚è«‹ç¢ºèª CSV å…§å®¹æ˜¯å¦å­˜åœ¨ã€‚")

    encodings = ["utf-8-sig", "utf-8", "cp950", "big5", "latin-1"]
    last_err = None
    for enc in encodings:
        try:
            bio = io.BytesIO(raw)
            return pd.read_csv(bio, encoding=enc)
        except Exception as e:
            last_err = e

    raise ValueError(f"è®€å– CSV å¤±æ•—ï¼ˆå·²å˜—è©¦ {encodings}ï¼‰ã€‚æœ€å¾ŒéŒ¯èª¤ï¼š{repr(last_err)}")


def safe_show_exception(e: Exception):
    st.error("ç™¼ç”ŸéŒ¯èª¤ï¼ˆsafeï¼‰")
    st.code(repr(e))
    with st.expander("Tracebackï¼ˆé™¤éŒ¯ç”¨ï¼‰"):
        st.code(traceback.format_exc())


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    """
    Excel-friendly: UTF-8 with BOM
    """
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


# ===== Item code detection =====
ITEM_CODE_RE = re.compile(r"^[A-Za-z]\d{2,3}(_\d+)?$")


def _find_item_cols(df: pd.DataFrame) -> list[str]:
    cols: list[str] = []
    for c in df.columns:
        s = str(c).strip()
        if ITEM_CODE_RE.match(s):
            cols.append(s)
    return cols


def _dim_letter(code: str) -> str | None:
    m = re.match(r"^([A-Za-z])", str(code))
    return m.group(1).upper() if m else None


def _nanmean_all_values(df_num: pd.DataFrame) -> float:
    arr = df_num.to_numpy(dtype=float)
    if arr.size == 0:
        return np.nan
    return float(np.nanmean(arr))


def build_code_and_dimmean_row(df_norm: pd.DataFrame) -> pd.DataFrame:
    """
    ç”¢ç”Ÿ 1 åˆ—å¯¬è¡¨ï¼ˆå–®åˆ—ï¼‰ï¼š
    - æ¯å€‹é¡Œé …ä»£ç¢¼æ¬„ä½ï¼šå„²å­˜æ ¼å€¼å¡«åŒä¸€å€‹ä»£ç¢¼ï¼ˆæ–¹ä¾¿æª¢æŸ¥/è¤‡è£½ï¼‰
    - æœ€å³å´ A/B/C... æ¬„ä½ï¼šå„æ§‹é¢æ•´é«”å¹³å‡ï¼ˆæ‰€æœ‰å—è©¦è€…Ã—è©²æ§‹é¢é¡Œé …æ”¤å¹³å¾Œå–å¹³å‡ï¼‰
    """
    item_cols_all = _find_item_cols(df_norm)
    if not item_cols_all:
        return pd.DataFrame()

    code_row = {c: c for c in item_cols_all}
    dims = sorted({d for d in (_dim_letter(c) for c in item_cols_all) if d is not None})

    dim_means = {}
    for d in dims:
        cols_d = [c for c in item_cols_all if _dim_letter(c) == d]
        df_d = df_norm[cols_d].apply(pd.to_numeric, errors="coerce")
        mean_d = _nanmean_all_values(df_d)
        dim_means[d] = f"{mean_d:.3f}" if np.isfinite(mean_d) else ""

    return pd.DataFrame([{**code_row, **dim_means}])


def build_dim_means_per_row(df_norm: pd.DataFrame) -> pd.DataFrame:
    """
    ç”¢ç”Ÿé€åˆ—ï¼ˆæ¯ä»½å•å·ä¸€åˆ—ï¼‰çš„æ§‹é¢å¹³å‡ï¼š
    - ä¾é¡Œé …ä»£ç¢¼ç¬¬ä¸€ç¢¼æ±ºå®šæ§‹é¢ï¼ˆA/B/C...ï¼‰
    - æ¯åˆ—å°è©²æ§‹é¢æ‰€æœ‰é¡Œç›®åš mean(axis=1, skipna=True)
    - è¼¸å‡ºç‚ºã€Œ4 ä½å°æ•¸å­—ä¸²ã€ï¼Œæœªæ»¿è£œ 0ï¼ˆä¾‹å¦‚ 3.5 â†’ 3.5000ï¼‰
    """
    item_cols_all = _find_item_cols(df_norm)
    if not item_cols_all:
        return pd.DataFrame()

    dims = sorted({d for d in (_dim_letter(c) for c in item_cols_all) if d is not None})

    df_item = df_norm[item_cols_all].apply(pd.to_numeric, errors="coerce")

    out = pd.DataFrame(index=df_norm.index)
    for d in dims:
        cols_d = [c for c in item_cols_all if _dim_letter(c) == d]
        mean_series = df_item[cols_d].mean(axis=1, skipna=True)
        out[d] = mean_series.apply(lambda x: f"{x:.4f}" if pd.notna(x) else "")

    return out


# ===== Regression table =====
def _sig_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def build_regression_table(df: pd.DataFrame, iv_vars: list[str], dv_var: str):
    """
    ç”¢ç”Ÿè¿´æ­¸è¡¨ï¼ˆæ¯”ç…§è«–æ–‡è¡¨æ ¼ï¼‰ï¼š
    - æœªæ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆbï¼›æ¬„åä»ç”¨ã€ŒÎ²ä¼°è¨ˆå€¼ã€ä»¥ç¬¦åˆä½ çš„è¡¨é ­ï¼‰
    - æ¨™æº–åŒ–ä¿‚æ•¸ Betaï¼ˆBeta = b * sd(x) / sd(y)ï¼‰
    - tã€é¡¯è‘—æ€§(p)
    - Fã€P(F)ã€RÂ²ã€Adj RÂ²ã€N
    """
    if not iv_vars or not dv_var:
        raise ValueError("è«‹å…ˆè¨­å®šè‡ªè®Šæ•¸èˆ‡ä¾è®Šæ•¸ã€‚")

    cols = iv_vars + [dv_var]
    d = df[cols].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    y = d[dv_var].astype(float)
    X = d[iv_vars].astype(float)
    Xc = sm.add_constant(X, has_constant="add")

    model = sm.OLS(y, Xc).fit()

    params = model.params
    tvals = model.tvalues
    pvals = model.pvalues

    sd_y = y.std(ddof=1)
    sd_x = X.std(ddof=1)

    beta_std = {}
    for v in iv_vars:
        if sd_y == 0 or pd.isna(sd_y) or sd_x[v] == 0 or pd.isna(sd_x[v]):
            beta_std[v] = np.nan
        else:
            beta_std[v] = params[v] * (sd_x[v] / sd_y)

    rows = []
    rows.append(
        {
            "è‡ªè®Šé …": "ï¼ˆå¸¸æ•¸ï¼‰",
            "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params['const']:.3f}",
            "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": "â€”",
            "t": f"{tvals['const']:.3f}{_sig_stars(pvals['const'])}",
            "é¡¯è‘—æ€§": f"{pvals['const']:.3f}",
        }
    )

    for v in iv_vars:
        rows.append(
            {
                "è‡ªè®Šé …": v,
                "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params[v]:.3f}",
                "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": ("" if pd.isna(beta_std[v]) else f"{beta_std[v]:.3f}"),
                "t": f"{tvals[v]:.3f}{_sig_stars(pvals[v])}",
                "é¡¯è‘—æ€§": f"{pvals[v]:.3f}",
            }
        )

    table_df = pd.DataFrame(rows)

    summary = {
        "F": float(model.fvalue) if model.fvalue is not None else np.nan,
        "P(F)": float(model.f_pvalue) if model.f_pvalue is not None else np.nan,
        "R2": float(model.rsquared),
        "Adj_R2": float(model.rsquared_adj),
        "N": int(model.nobs),
    }
    return table_df, summary


# ---- Sidebar ----
with st.sidebar:
    st.header("è¨­å®š")
    st.caption("1) ä¸Šå‚³ CSV â†’ 2) ç”¢å‡º Item Analysis â†’ 3) ä¸‹è¼‰çµæœï¼ˆCSVï¼‰")

    uploaded_file = st.file_uploader("ä¸Šå‚³ CSV", type=["csv"])

    st.divider()
    st.subheader("GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆå¯é¸ï¼‰")

    gpt_on = st.toggle("å•Ÿç”¨ GPT å ±å‘Š", value=False, help="éœ€è¦ OpenAI API Key èˆ‡å¯ç”¨é¡åº¦ï¼ˆquotaï¼‰ã€‚")

    model_options = ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o", "gpt-4.1"]
    model_pick = st.selectbox("é¸æ“‡ GPT æ¨¡å‹", options=model_options, index=0)
    model_custom = st.text_input("æˆ–è‡ªè¡Œè¼¸å…¥æ¨¡å‹åç¨±ï¼ˆé¸å¡«ï¼‰", value="", placeholder="ä¾‹å¦‚ï¼šgpt-4o-mini")
    model_name = (model_custom.strip() or model_pick).strip()

    api_key = st.text_input("OpenAI API Keyï¼ˆä»¥ sk- é–‹é ­ï¼‰", type="password", value="")
    st.caption("å»ºè­°ç”¨ç’°å¢ƒè®Šæ•¸ä¹Ÿå¯ï¼šå…ˆåœ¨ç³»çµ±è¨­å®š OPENAI_API_KEYï¼Œå†ç•™ç©ºæ­¤æ¬„ã€‚")

    st.divider()
    st.subheader("å­æ§‹é¢è¦å‰‡ï¼ˆä½ æŒ‡å®šï¼‰")
    st.write("å­æ§‹é¢åªå–é¡Œé …ä»£ç¢¼çš„**å‰å…©ç¢¼**ï¼šä¾‹å¦‚ A01â†’A0ã€A11â†’A1ã€A105â†’A1")
    st.caption("â€» é€™å€‹è¦å‰‡éœ€ç”± analysis.py çš„åˆ†ç¾¤é‚è¼¯é…åˆï¼ˆè‹¥ä½ å·²æ”¹å¥½ analysis.py å°±æœƒç”Ÿæ•ˆï¼‰ã€‚")


# ---- Main ----
if uploaded_file is None:
    st.info("è«‹å…ˆåœ¨å·¦å´ä¸Šå‚³ CSV æª”æ¡ˆã€‚")
    st.stop()

try:
    df_raw = read_csv_safely(uploaded_file)
except Exception as e:
    safe_show_exception(e)
    st.stop()

# æ­£è¦åŒ–æ¬„åï¼ˆæ”¯æ´ A01.é¡Œç›® / A01 é¡Œç›® / A01ï¼‰
df_norm, mapping = normalize_item_columns(df_raw)

st.subheader("åŸå§‹è³‡æ–™é è¦½ï¼ˆå‰ 5 åˆ—ï¼‰")
st.dataframe(df_raw.head(), width="stretch")

with st.expander("æ¬„åæ­£è¦åŒ–å°ç…§ï¼ˆåŸå§‹æ¬„å â†’ é¡Œé …ä»£ç¢¼ï¼‰"):
    if mapping:
        map_df = pd.DataFrame([{"åŸå§‹æ¬„å": k, "é¡Œé …ä»£ç¢¼": v} for k, v in mapping.items()])
        st.dataframe(map_df, width="stretch")
    else:
        st.write("æœªåµæ¸¬åˆ°å¯æ­£è¦åŒ–çš„é¡Œé …æ¬„åï¼ˆè«‹ç¢ºèªæ¬„åæ ¼å¼ï¼‰ã€‚")

# ---- Item Analysis ----
st.subheader("ğŸ“ˆ Item Analysis çµæœ")

try:
    # 1) Item analysis
    result_df = run_item_analysis(df_norm)
    st.success("Item analysis completed.")
    st.dataframe(result_df, width="stretch", height=520)

    st.download_button(
        "ä¸‹è¼‰ Item Analysis çµæœ CSV",
        data=df_to_csv_bytes(result_df),
        file_name="item_analysis_results.csv",
        mime="text/csv",
    )


    # 2) é€åˆ—ï¼šæ¯ä»½å•å·ä¸€åˆ—çš„æ§‹é¢å¹³å‡ï¼ˆA/B/C/D...ï¼›4ä½å°æ•¸è£œ0ï¼‰
    st.markdown("### å„æ§‹é¢å¹³å‡ï¼ˆé€åˆ—ï¼æ¯ä»½å•å·ä¸€åˆ—ï¼‰")
    df_dim_means_row = build_dim_means_per_row(df_norm)
    if df_dim_means_row.empty:
        st.warning("æ‰¾ä¸åˆ°é¡Œé …ä»£ç¢¼æ¬„ä½ï¼Œç„¡æ³•ç”¢ç”Ÿã€é€åˆ—æ§‹é¢å¹³å‡ã€ã€‚")
        st.stop()

    st.dataframe(df_dim_means_row, width="stretch", height=360)
    st.download_button(
        "ä¸‹è¼‰ é€åˆ—æ§‹é¢å¹³å‡ CSV",
        data=df_to_csv_bytes(df_dim_means_row),
        file_name="dim_means_by_row.csv",
        mime="text/csv",
    )



    # 3) åŸå§‹é€ç­† + æ§‹é¢å¹³å‡ï¼ˆé€åˆ—ï¼‰
    st.markdown("### åŸå§‹é€ç­†è³‡æ–™ + æ§‹é¢å¹³å‡ï¼ˆé€åˆ—ï¼‰")
    df_raw_plus_dimmeans = df_norm.copy()
    for c in df_dim_means_row.columns:
        df_raw_plus_dimmeans[c] = df_dim_means_row[c]

    st.dataframe(df_raw_plus_dimmeans, width="stretch", height=520)
    st.download_button(
        "ä¸‹è¼‰ åŸå§‹é€ç­†+æ§‹é¢å¹³å‡ CSV",
        data=df_to_csv_bytes(df_raw_plus_dimmeans),
        file_name="raw_plus_dim_means_by_row.csv",
        mime="text/csv",
    )

    # 4) ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆIV / DVï¼‰+ è¿´æ­¸åˆ†æè¡¨
    st.divider()
    st.subheader("ğŸ“Œ ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆè‡ªè®Šæ•¸ / ä¾è®Šæ•¸ï¼‰")

    dim_cols = list(df_dim_means_row.columns)  # A, B, C, D ...

    iv_vars = st.multiselect(
        "â‘  å‹¾é¸è‡ªè®Šæ•¸ï¼ˆå¯è¤‡é¸ï¼‰",
        options=dim_cols,
        default=[],
    )

    dv_var = st.selectbox(
        "â‘¡ é¸æ“‡ä¾è®Šæ•¸ï¼ˆå–®ä¸€ï¼‰",
        options=[""] + dim_cols,
        index=0,
    )

    if dv_var and dv_var in iv_vars:
        st.error("âš ï¸ ä¾è®Šæ•¸ä¸å¯åŒæ™‚è¢«é¸ç‚ºè‡ªè®Šæ•¸ï¼Œè«‹é‡æ–°è¨­å®šã€‚")
    elif iv_vars and dv_var:
        st.success(f"ç ”ç©¶æ¨¡å‹ï¼šIV = {iv_vars} â†’ DV = {dv_var}")

        st.markdown("### â‘¢ ç ”ç©¶ç”¨è³‡æ–™è¡¨ï¼ˆåƒ…ä¿ç•™ IV / DVï¼‰")
        selected_cols = iv_vars + [dv_var]
        df_research = df_raw_plus_dimmeans[selected_cols].copy()

        st.dataframe(df_research, width="stretch")
        st.download_button(
            "ä¸‹è¼‰ ç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + DVï¼‰",
            data=df_to_csv_bytes(df_research),
            file_name="research_dataset_IV_DV.csv",
            mime="text/csv",
        )

        st.divider()
        st.subheader("ğŸ“Š è¿´æ­¸åˆ†æè¡¨ï¼ˆè«–æ–‡æ ¼å¼ï¼‰")

        iv_list = "ã€".join(iv_vars)
        title = f"è‡ªè®Šæ•¸ï¼ˆ{iv_list}ï¼‰å° ä¾è®Šæ•¸ {dv_var} ä¹‹è¿´æ­¸åˆ†æè¡¨"
        st.markdown(f"### {title}")

        run_reg = st.button("åŸ·è¡Œè¿´æ­¸åˆ†æ", type="primary")

        if run_reg:
            try:
                reg_table, reg_sum = build_regression_table(df_research, iv_vars, dv_var)

                st.dataframe(reg_table, width="stretch")

                st.markdown(
                    f"**F={reg_sum['F']:.3f}ï¼ŒP={reg_sum['P(F)']:.3f}ï¼ŒR å¹³æ–¹={reg_sum['R2']:.3f}ï¼Œ"
                    f"èª¿æ•´å¾Œçš„ R å¹³æ–¹={reg_sum['Adj_R2']:.3f}ï¼ˆN={reg_sum['N']}ï¼‰**"
                )
                st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001")

                file_tag = f"{'+'.join(iv_vars)}_to_{dv_var}".replace(" ", "")
                st.download_button(
                    "ä¸‹è¼‰ è¿´æ­¸åˆ†æè¡¨ CSV",
                    data=df_to_csv_bytes(reg_table),
                    file_name=f"regression_table_{file_tag}.csv",
                    mime="text/csv",
                )

            except Exception as e:
                st.error("è¿´æ­¸åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
                safe_show_exception(e)

    else:
        st.info("è«‹å…ˆé¸æ“‡è‡³å°‘ä¸€å€‹è‡ªè®Šæ•¸èˆ‡ä¸€å€‹ä¾è®Šæ•¸ï¼Œæ‰æœƒç”¢å‡ºç ”ç©¶ç”¨è³‡æ–™èˆ‡è¿´æ­¸è¡¨æ ¼ã€‚")

except Exception as e:
    st.error("Item analysis failed. See error details below (safe).")
    safe_show_exception(e)
    st.stop()

# ---- GPT report (optional) ----
st.divider()
st.subheader("ğŸ“ GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆæ–‡å­—ï¼‰")

if not gpt_on:
    st.info("ä½ ç›®å‰æœªå•Ÿç”¨ GPT å ±å‘Šã€‚è‹¥è¦ç”Ÿæˆè«–æ–‡æ–‡å­—ï¼Œè«‹åœ¨å·¦å´æ‰“é–‹ã€Œå•Ÿç”¨ GPT å ±å‘Šã€ã€‚")
    st.stop()

if not GPT_AVAILABLE:
    st.warning("æ‰¾ä¸åˆ°å¯ç”¨çš„ generate_gpt_reportï¼ˆè«‹ç¢ºèª gpt_report.py ä¸­æœ‰å®šç¾© generate_gpt_reportï¼‰ã€‚")
    st.stop()

key = (api_key or os.getenv("OPENAI_API_KEY") or "").strip()
if not key:
    st.warning("å°šæœªæä¾› OpenAI API Keyã€‚è«‹åœ¨å·¦å´è¼¸å…¥ï¼Œæˆ–è¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()

gen = st.button("ç”Ÿæˆ GPT å ±å‘Šï¼ˆæ–‡å­—ï¼‰", type="primary")

if gen:
    try:
        report = generate_gpt_report(result_df, model=model_name, api_key=key)

        paper_text = None
        if isinstance(report, dict):
            paper_text = report.get("paper_text") or report.get("text") or report.get("output")
        elif isinstance(report, str):
            paper_text = report

        if not paper_text:
            st.warning("GPT å›å‚³å…§å®¹ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ gpt_report.py çš„å›å‚³æ ¼å¼ã€‚")
        else:
            st.success("GPT å ±å‘Šç”Ÿæˆå®Œæˆã€‚")
            st.text_area("GPT è«–æ–‡å ±å‘Šï¼ˆå¯è¤‡è£½ï¼‰", value=paper_text, height=420)

            st.download_button(
                "ä¸‹è¼‰ GPT å ±å‘Š TXT",
                data=paper_text.encode("utf-8"),
                file_name="gpt_paper_report.txt",
                mime="text/plain",
            )

    except Exception as e:
        msg = repr(e)
        if "insufficient_quota" in msg or "You exceeded your current quota" in msg:
            st.error("GPT report failedï¼šä½ çš„ OpenAI API å¸³è™Ÿç›®å‰æ²’æœ‰å¯ç”¨é¡åº¦ï¼ˆinsufficient_quotaï¼‰ã€‚")
            st.caption("è§£æ³•ï¼šåˆ° OpenAI å¹³å° Billing/Credits åŠ å€¼å¾Œå†è©¦ã€‚")
        else:
            st.error("GPT report failed. See error details below (safe).")
            safe_show_exception(e)
